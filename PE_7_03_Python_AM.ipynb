{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Exercise 7.03: Carhart 4-Factor Model"
      ],
      "metadata": {
        "id": "aDxYLagFTGrz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaEn2FglTDEo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "import yfinance as yf\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Download Carhart factors (Fama-French 3 factors + Momentum)\n",
        "def download_carhart_factors():\n",
        "    ff_url = 'https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_daily_CSV.zip'\n",
        "    response_ff = requests.get(ff_url)\n",
        "    with zipfile.ZipFile(BytesIO(response_ff.content)) as z:\n",
        "        file_name = z.namelist()[0]\n",
        "        with z.open(file_name) as f:\n",
        "            ff_raw_lines = f.read().decode('utf-8').splitlines()\n",
        "            ff_data_start = next((i for i, line in enumerate(ff_raw_lines) if line.strip() and line[0].isdigit()), None)\n",
        "            ff_cleaned_data = '\\n'.join(ff_raw_lines[ff_data_start:])\n",
        "            fama_french_factors = pd.read_csv(\n",
        "                BytesIO(ff_cleaned_data.encode('utf-8')),\n",
        "                index_col=0, header=None, names=['Mkt-RF', 'SMB', 'HML', 'RF'], skip_blank_lines=True\n",
        "            )\n",
        "\n",
        "    mom_url = 'https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Momentum_Factor_daily_CSV.zip'\n",
        "    response_mom = requests.get(mom_url)\n",
        "    with zipfile.ZipFile(BytesIO(response_mom.content)) as z:\n",
        "        file_name = z.namelist()[0]\n",
        "        with z.open(file_name) as f:\n",
        "            mom_raw_lines = f.read().decode('utf-8').splitlines()\n",
        "            mom_data_start = next((i for i, line in enumerate(mom_raw_lines) if line.strip() and line[0].isdigit()), None)\n",
        "            mom_cleaned_data = '\\n'.join(mom_raw_lines[mom_data_start:])\n",
        "            momentum_factor = pd.read_csv(\n",
        "                BytesIO(mom_cleaned_data.encode('utf-8')),\n",
        "                index_col=0, header=None, names=['MOM'], skip_blank_lines=True\n",
        "            )\n",
        "\n",
        "    fama_french_factors = fama_french_factors[fama_french_factors.index.astype(str).str.match(r'^\\d{8}$')]\n",
        "    fama_french_factors.index = pd.to_datetime(fama_french_factors.index, format='%Y%m%d', errors='coerce')\n",
        "    fama_french_factors = fama_french_factors.dropna()\n",
        "\n",
        "    momentum_factor = momentum_factor[momentum_factor.index.astype(str).str.match(r'^\\d{8}$')]\n",
        "    momentum_factor.index = pd.to_datetime(momentum_factor.index, format='%Y%m%d', errors='coerce')\n",
        "    momentum_factor = momentum_factor.dropna()\n",
        "\n",
        "    carhart_factors = pd.merge(fama_french_factors, momentum_factor, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "    return carhart_factors\n",
        "\n",
        "def download_stock_data(symbols, start_date):\n",
        "    data = yf.download(symbols, start=start_date, auto_adjust=False, actions=False)['Adj Close']\n",
        "    return data.pct_change().dropna() * 100\n",
        "\n",
        "def calculate_betas(stock_returns, factors):\n",
        "    betas = pd.DataFrame(index=stock_returns.columns, columns=['Alpha'] + list(factors.columns.drop('RF')))\n",
        "    for stock in stock_returns.columns:\n",
        "        Y = stock_returns[stock].dropna()\n",
        "        X = factors.drop(columns=['RF']).loc[Y.index].dropna()\n",
        "\n",
        "        if X.empty or Y.empty:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            X = sm.add_constant(X)\n",
        "            model = sm.OLS(Y, X).fit()\n",
        "            betas.loc[stock, 'Alpha'] = model.params['const']\n",
        "            for factor in factors.columns.drop('RF'):\n",
        "                betas.loc[stock, factor] = model.params.get(factor, None)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {stock}: {e}\")\n",
        "\n",
        "    return betas.dropna()\n",
        "\n",
        "def calculate_annualized_returns(betas, factors):\n",
        "    avg_factors = factors.mean()\n",
        "    trading_days = 252\n",
        "    avg_factors_annualized = avg_factors * trading_days\n",
        "\n",
        "    annualized_returns = pd.DataFrame(index=betas.index, columns=['Total Return'])\n",
        "    for stock in betas.index:\n",
        "        rf_annualized = factors['RF'].mean() * trading_days\n",
        "        factor_contributions = (betas.loc[stock, betas.columns[1:]] * avg_factors_annualized[betas.columns[1:]]).sum()\n",
        "        total_return = rf_annualized + factor_contributions\n",
        "\n",
        "        annualized_returns.loc[stock, 'Total Return'] = total_return\n",
        "\n",
        "    return annualized_returns\n",
        "\n",
        "def decompose_returns(betas, factors):\n",
        "    avg_factors = factors.mean()\n",
        "    trading_days = 252\n",
        "    avg_factors_annualized = avg_factors * trading_days\n",
        "\n",
        "    decomposition = pd.DataFrame(index=betas.index, columns=factors.columns.drop('RF').tolist() + ['RF', 'Total'])\n",
        "    for stock in betas.index:\n",
        "        rf_annualized = factors['RF'].mean() * trading_days\n",
        "        factor_contributions = betas.loc[stock, betas.columns[1:]] * avg_factors_annualized[betas.columns[1:]]\n",
        "        total_return = rf_annualized + factor_contributions.sum()\n",
        "\n",
        "        decomposition.loc[stock, factors.columns.drop('RF')] = factor_contributions\n",
        "        decomposition.loc[stock, 'RF'] = rf_annualized\n",
        "        decomposition.loc[stock, 'Total'] = total_return\n",
        "\n",
        "    return decomposition\n",
        "\n",
        "symbols = ['AAPL', 'AMZN', 'META', 'GOOGL', 'MSFT', 'NVDA', 'TSLA']\n",
        "start_date = '2014-01-01'\n",
        "\n",
        "carhart_factors = download_carhart_factors()\n",
        "stock_returns = download_stock_data(symbols, start_date)\n",
        "\n",
        "stock_returns.index = stock_returns.index.tz_localize(None)\n",
        "carhart_factors.index = carhart_factors.index.tz_localize(None)\n",
        "carhart_factors = carhart_factors.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "common_dates = stock_returns.index.intersection(carhart_factors.index)\n",
        "stock_returns = stock_returns.loc[common_dates]\n",
        "carhart_factors = carhart_factors.loc[common_dates]\n",
        "\n",
        "stock_returns = stock_returns.dropna()\n",
        "carhart_factors = carhart_factors.dropna()\n",
        "\n",
        "betas = calculate_betas(stock_returns, carhart_factors)\n",
        "annualized_returns = calculate_annualized_returns(betas, carhart_factors)\n",
        "returns_decomposition = decompose_returns(betas, carhart_factors)\n",
        "\n",
        "# Print results\n",
        "print(\"Betas (including Alpha):\")\n",
        "print(betas)\n",
        "\n",
        "print(\"\\nAnnualized Returns:\")\n",
        "print(annualized_returns)\n",
        "\n",
        "print(\"\\nReturns Decomposition:\")\n",
        "print(returns_decomposition)\n"
      ]
    }
  ]
}