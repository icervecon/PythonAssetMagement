{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Exercise 7.05: Analyzing Factor Portfolios"
      ],
      "metadata": {
        "id": "qHz_UMUyTqBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST9KNoALTmHg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "import statsmodels.formula.api as smf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from io import StringIO\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FF Data Library file download\n",
        "\n",
        "url = 'https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/25_Portfolios_5x5_daily_CSV.zip'\n",
        "\n",
        "# Specify the path where you want the file to be saved\n",
        "zip_path = '25_Portfolios_5x5_daily.zip'\n",
        "\n",
        "# Make HTTP request to download the file\n",
        "response = requests.get(url)\n",
        "with open(zip_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "            print(f\"File downloaded and saved as: {zip_path}\")\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    # List the names of files in the ZIP to identify the CSV\n",
        "    csv_files = [f for f in z.namelist() if f.endswith('.csv')]\n",
        "    csv_file_name = csv_files[0]\n",
        "    z.extract(csv_file_name, '.')\n",
        "    print(f\"File '{csv_file_name}'extracted correctly.\")\n",
        "\n",
        "#Review the contents of the file\n",
        "with open('25_Portfolios_5x5_Daily.csv', 'r') as file:\n",
        "    for i in range(20):\n",
        "        print(i, file.readline().strip())\n",
        "\n",
        "# Load the CSV file, assuming the first column should be\n",
        "# named 'Date' and skipping the header\n",
        "\n",
        "filepath='25_Portfolios_5x5_Daily.csv'\n",
        "portfolios = pd.read_csv(filepath, skiprows=18, low_memory=False)\n",
        "portfolios.rename(columns={portfolios.columns[0]: 'Date'}, inplace=True)\n",
        "\n",
        "# Remove the first row that is duplicated from the headings\n",
        "df_portfolios = portfolios.drop(0)\n",
        "\n",
        "# Convert column 'Date' to datetime\n",
        "df_portfolios['Date'] = pd.to_datetime(df_portfolios['Date'], errors='coerce')\n",
        "\n",
        "# Attempt to convert numeric columns to float, errors will\n",
        "# be handled as NaN\n",
        "for col in df_portfolios.columns[1:]:\n",
        "    df_portfolios[col] = pd.to_numeric(df_portfolios[col], errors='coerce')\n",
        "\n",
        " # Find the index of the first completely empty row\n",
        "first_empty_index = df_portfolios[df_portfolios.isnull().all(axis=1)].index.min()\n",
        "\n",
        "# If there is a completely empty row, extract all rows up to that row.\n",
        "if pd.notna(first_empty_index):\n",
        "    df_portfolios = df_portfolios.iloc[:first_empty_index-1]\n",
        "else:\n",
        "    df_portfolios = df_portfolios   # If there are no empty rows, use the whole DataFrame\n",
        "# Create a replacement dictionary\n",
        "replacement = {\n",
        "    'SMALL LoBM': 'ME1 BM1',\n",
        "    'SMALL HiBM': 'ME1 BM5',\n",
        "    'BIG LoBM': 'ME5 BM1',\n",
        "    'BIG HiBM': 'ME5 BM5',\n",
        "   }\n",
        "\n",
        "# Rename headings\n",
        "df_portfolios.rename(columns=replacement, inplace=True)\n",
        "#Filtering data by date\n",
        "df_portfolios['Date'] = pd.to_datetime(df_portfolios['Date'])\n",
        "start_date = pd.to_datetime('1963-07-01')\n",
        "end_date = pd.to_datetime('2024-05-31')\n",
        "try:\n",
        "    filtered_portfolios = df_portfolios[(df_portfolios['Date'] >= start_date) & (df_portfolios['Date'] <= end_date)]\n",
        "except TypeError as e:\n",
        "    print(\"TypeError encountered:\", e)\n",
        "    print(\"Re-checking the data types...\")\n",
        "    print(portfolios.dtypes)  # This will help identify if there's still a data type issue.\n"
      ],
      "metadata": {
        "id": "TNT5AO5XTy8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FF Data Library file download\n",
        "\n",
        "url = 'https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_daily_csv.zip'\n",
        "\n",
        "# Specify the path where you want the file to be saved\n",
        "zip_path2 = 'F-F_Research_Data_5_Factors_2x3_daily_csv.zip'\n",
        "\n",
        "# Make HTTP request to download the file\n",
        "response = requests.get(url)\n",
        "with open(zip_path2, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "            print(f\"File downloaded and saved as: {zip_path2}\")\n",
        "\n",
        "with zipfile.ZipFile(zip_path2, 'r') as z:\n",
        "\n",
        "    # List the names of files in the ZIP to identify the CSV\n",
        "    csv_files2 = [f for f in z.namelist() if f.endswith('.csv')]\n",
        "    csv_file_name2 = csv_files2[0]\n",
        "    z.extract(csv_file_name2, '.')\n",
        "    print(f\"File '{csv_file_name2}' extracted correctly.\")\n"
      ],
      "metadata": {
        "id": "mX8JqL6wT2Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Review the contents of the file\n",
        "with open('F-F_Research_Data_5_Factors_2x3_daily.csv', 'r') as file:\n",
        "    for i in range(20):\n",
        "        print(i, file.readline().strip())\n"
      ],
      "metadata": {
        "id": "9UgJdvkCT34E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file, assuming the first column should be named 'Date' and skipping the header\n",
        "csv_name = 'F-F_Research_Data_5_Factors_2x3_daily.csv'\n",
        "\n",
        "# Load CSV robustly: detect header row and add 'Date' ---\n",
        "with open(csv_name, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "    lines = f.read().splitlines()\n",
        "\n",
        "# Find the true header: line that contains 'Mkt-RF' and starts with a comma after optional spaces\n",
        "header_idx = next(\n",
        "    i for i, l in enumerate(lines)\n",
        "    if \"Mkt-RF\" in l and l.lstrip().startswith(\",\")\n",
        ")\n",
        "\n",
        "# Prepend 'Date' to the header (turn \",Mkt-RF,...\" into \"Date,Mkt-RF,...\")\n",
        "header_line = \"Date\" + lines[header_idx]\n",
        "\n",
        "# Build a CSV string from header + all subsequent lines\n",
        "csv_text = \"\\n\".join([header_line] + lines[header_idx + 1:])\n",
        "\n",
        "# Parse; skip initial spaces after commas\n",
        "raw = pd.read_csv(\n",
        "    StringIO(csv_text),\n",
        "    skip_blank_lines=True,\n",
        "    skipinitialspace=True,\n",
        ")\n",
        "\n",
        "\n",
        "# Clean: keep only YYYYMMDD rows, convert types ---\n",
        "raw.columns = raw.columns.str.strip()\n",
        "factors = raw[raw[\"Date\"].astype(str).str.match(r\"^\\s*\\d{8}\\s*$\", na=False)].copy()\n",
        "\n",
        "# Convert Date to datetime\n",
        "factors[\"Date\"] = pd.to_datetime(\n",
        "    factors[\"Date\"].astype(str).str.strip(),\n",
        "    format=\"%Y%m%d\",\n",
        "    errors=\"coerce\"\n",
        ")\n",
        "factors = factors.dropna(subset=[\"Date\"])\n",
        "\n",
        "# Ensure factor columns are numeric\n",
        "factor_cols = [\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"RF\"]\n",
        "missing = [c for c in factor_cols if c not in factors.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing expected columns: {missing}\")\n",
        "for c in factor_cols:\n",
        "    factors[c] = pd.to_numeric(factors[c], errors=\"coerce\")\n",
        "\n",
        "# --- 5) (Optional) rename, then filter by date range ---\n",
        "replacement = {\"Mkt-RF\":\"Market\",\"SMB\":\"Size\",\"HML\":\"Value\",\"RMW\":\"Quality\",\"CMA\":\"Investment\"}\n",
        "factors = factors.rename(columns=replacement)\n",
        "\n",
        "start_date = pd.to_datetime(\"19630701\", format=\"%Y%m%d\")\n",
        "end_date   = pd.to_datetime(\"20240531\", format=\"%Y%m%d\")\n",
        "filtered_factors = factors[(factors[\"Date\"] >= start_date) & (factors[\"Date\"] <= end_date)].copy()\n",
        "\n",
        "print(\"\\nFiltered factors (tail):\")\n",
        "print(filtered_factors.tail())\n"
      ],
      "metadata": {
        "id": "J49kEr2cT6_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We combine the tables based on the column 'date'.\n",
        "\n",
        "df1 = pd.DataFrame(filtered_portfolios)\n",
        "df1['Date'] = pd.to_datetime(df1['Date'])\n",
        "\n",
        "df2 = pd.DataFrame(filtered_factors)\n",
        "df2['Date'] = pd.to_datetime(df2['Date'])\n",
        "\n",
        "df_combined = pd.merge(df1, df2, on='Date', how='outer')\n",
        "\n",
        "#Sort by date\n",
        "df_combined = df_combined.sort_values(by='Date')\n",
        "\n",
        "df_combined.tail()\n"
      ],
      "metadata": {
        "id": "GjRvOsx8UBr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculation of annual volatility\n",
        "\n",
        "#Ensure that the date column is in the index.\n",
        "\n",
        "df_combined.set_index('Date', inplace=True)\n",
        "\n",
        "#Ensure that all columns except 'date' are numeric\n",
        "for col in df_combined.columns:\n",
        "    if col != 'Date':\n",
        "        df_combined[col] = pd.to_numeric(df_combined[col], errors='coerce')\n",
        "\n",
        "#Excluded columns\n",
        "excluded_columns= ['Market', 'Size', 'Value','Quality', 'Investment', 'RF']\n",
        "\n",
        "#Select the columns you want to process\n",
        "columns_to_process = [col for col in df_combined.columns if col not in excluded_columns]\n",
        "\n",
        "#Calculate daily volatility\n",
        "daily_volatility = df_combined[columns_to_process].std()\n",
        "\n",
        "#Convert daily volatility to annual volatility\n",
        "annual_volatility = daily_volatility * np.sqrt(252)\n",
        "\n",
        "#Create a DataFrame for the annual volatility row\n",
        "summary_volatility = pd.DataFrame(annual_volatility, columns=['volatility_annual']).T\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Ensure that all columns are present and in the\n",
        "# same order as the original DataFrame.\n",
        "summary_volatility = summary_volatility.reindex(columns=df_combined.columns)\n",
        "\n",
        "#Concatenate results to original DataFrame\n",
        "df_combined_volatility = pd.concat([df_combined, summary_volatility])\n"
      ],
      "metadata": {
        "id": "5DaBYfdCUFMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Abnormal Returns\n",
        "#Create a copy for the result\n",
        "df_abnormal_returns= df_combined.copy()\n",
        "\n",
        "#Select the reference column\n",
        "col_ref = 'RF'\n",
        "\n",
        "#Indicate the columns to be excluded\n",
        "exclude_columns = ['Market', 'Size', 'Value','Quality', 'Investment']\n",
        "\n",
        "\n",
        "#Subtract the reference column from all other numeric columns\n",
        "for column in df_combined.select_dtypes(include='number').columns:\n",
        "    if column != col_ref and column not in exclude_columns:\n",
        "        df_abnormal_returns[column] = df_combined[column] - df_combined[col_ref]\n"
      ],
      "metadata": {
        "id": "L0K-hNevUIXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculation of annual abnormal returns and Sharpe ratio\n",
        "\n",
        "#Columns to be excluded in the calculation\n",
        "excluded_columns= ['Market', 'Size', 'Value','Quality', 'Investment', 'RF']\n",
        "\n",
        "#Select the columns you want to process\n",
        "columns_to_process = [col for col in df_abnormal_returns.columns if col not in excluded_columns]\n",
        "\n",
        "# Calculate the abnormal returns daily average\n",
        "# for the selected columns\n",
        "daily_mean = df_abnormal_returns[columns_to_process].mean()\n",
        "\n",
        "#Convert to annual average\n",
        "annual_mean = daily_mean * 252\n",
        "Sharpe_ratio=annual_mean/annual_volatility\n",
        "\n",
        "#Create series with the results and keep the original portfolio names.\n",
        "summary_mean = pd.Series(annual_mean, name='AbnRet mean annual')\n",
        "summary_volatility = pd.Series(annual_volatility, name='Volatility annual')\n",
        "summary_Sharpe=pd.Series(Sharpe_ratio, name='Sharpe')\n",
        "\n",
        "#Add the excluded columns with NaN\n",
        "for col in exclude_columns:\n",
        "    summary_mean[col] = np.nan\n",
        "    summary_volatility[col] = np.nan\n",
        "    summary_Sharpe[col]=np.nan\n",
        "\n",
        "#Convert the Series to DataFrames and reorder the columns\n",
        "# to match the original DataFrame.\n",
        "summary_mean = summary_mean.reindex(df_abnormal_returns.columns).to_frame().T\n",
        "summary_volatility = summary_volatility.reindex(df_combined_volatility.columns).to_frame().T\n",
        "summary_Sharpe=summary_Sharpe.reindex(df_abnormal_returns.columns).to_frame().T\n",
        "\n",
        "#Delete rows of previous results if they exist\n",
        "df_abnormal_returns= df_abnormal_returns.drop(index=['AbnRet mean annual', 'Volatility annual', 'Sharpe'], errors='ignore')\n",
        "\n",
        "\n",
        "#Concatenate results to original DataFrame\n",
        "df_abnormal_returns = pd.concat([df_abnormal_returns, summary_mean, summary_volatility, summary_Sharpe])\n",
        "\n",
        "#Convert the date index to a simplified string format only\n",
        "# if they are not summary indexes\n",
        "df_abnormal_returns.index = df_abnormal_returns.index.map(lambda x: x.strftime('%Y-%m-%d') if isinstance(x, pd.Timestamp) else x)\n",
        "df_abnormal_returns.tail()\n"
      ],
      "metadata": {
        "id": "zua9petfUWJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume df_combined is your already loaded DataFrame\n",
        "df = df_combined.copy()\n",
        "\n",
        "# Clean up column names to ensure they have no spaces or special characters.\n",
        "df.columns = df.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
        "\n",
        "# Assume that the first 25 columns are the dependent columns and the last 5 are the independent columns.\n",
        "dependent_columns = df.columns[:25]\n",
        "independent_columns = df.columns[25:30]\n",
        "\n",
        "\n",
        "# Generate regression formulas\n",
        "formulas = [f'{dep} ~ {\" + \".join(independent_columns)}' for dep in dependent_columns]\n",
        "\n",
        "# Perform regressions and extract coefficients and p-values.\n",
        "results = {}\n",
        "for formula in formulas:\n",
        "    model = smf.ols(formula=formula, data=df).fit()\n",
        "    params = model.params\n",
        "    pvalues = model.pvalues\n",
        "    results[formula] = {'coefficients': params, 'pvalues': pvalues}\n",
        "\n",
        "# Show coefficients and p-values\n",
        "for formula, result in results.items():\n",
        "    print(f'Resultados para la fÃ³rmula \"{formula}\":')\n",
        "    print('Coeficientes:')\n",
        "    print(result['coefficients'])\n",
        "    print('P-values:')\n",
        "    print(result['pvalues'])\n",
        "    print('\\n')\n"
      ],
      "metadata": {
        "id": "u_m2sfK7UcIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation of dictionaries to store coefficients and p-values separately\n",
        "coefficients = {}\n",
        "pvalues = {}\n",
        "\n",
        "# Run regressions and extract coefficients and p-values\n",
        "for formula in formulas:\n",
        "    model = smf.ols(formula=formula, data=df).fit()\n",
        "    dep = formula.split('~')[0].strip()  # Obtener el nombre de la variable dependiente\n",
        "    coefficients[dep] = model.params\n",
        "    pvalues[dep] = model.pvalues\n",
        "\n",
        "# Converting dictionaries to DataFrames\n",
        "coefficients_df = pd.DataFrame(coefficients)\n",
        "pvalues_df = pd.DataFrame(pvalues)\n",
        "\n",
        "# Simplifying column names and replacing underscores with spaces\n",
        "coefficients_df.columns = [col.replace('_', ' ') for col in coefficients_df.columns]\n",
        "pvalues_df.columns = [col.replace('_', ' ') for col in pvalues_df.columns]\n"
      ],
      "metadata": {
        "id": "dPX09PHPUfPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the final DataFrame ensuring that the above data does not accumulate\n",
        "df_abnormal_returns= df_abnormal_returns.drop(index=['AbnRet mean annual', 'Volatility annual', 'Sharpe'], errors='ignore')\n",
        "df_updated = pd.concat([df_abnormal_returns, summary_mean, summary_volatility, summary_Sharpe, coefficients_df])\n",
        "\n",
        "# Ensure the index is called 'Date'.\n",
        "if df_updated.index.name != 'Date':\n",
        "    df_updated.index.name = 'Date'\n",
        "df_updated.tail(11)\n"
      ],
      "metadata": {
        "id": "fHCnWaBQUjcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the first 25 relevant columns\n",
        "df_filtered = df_updated.iloc[:, :25]\n",
        "\n",
        "# Define the metrics to create individual DataFrames and\n",
        "# their respective pivots\n",
        "metrics = [\n",
        "    'AbnRet mean annual', 'Volatility annual', 'Sharpe', 'Intercept',\n",
        "    'Market', 'Size', 'Value', 'Quality', 'Investment'\n",
        "]\n",
        "\n",
        "# Dictionary to store the pivot DataFrames\n",
        "pivot_dfs = {}\n",
        "\n",
        "# Process each metric, extract data, create pivot and\n",
        "# store in dictionary\n",
        "for metric in metrics:\n",
        "    temp_df = df_filtered.loc[metric].reset_index()\n",
        "    temp_df[['ME', 'BM']] = temp_df['index'].str.split(' ', expand=True)\n",
        "    temp_df.drop(columns=['index'], inplace=True)\n",
        "    pivot_dfs[metric] = temp_df.pivot(index='BM', columns='ME', values=metric)\n",
        "\n",
        "# Print each rounded DataFrame pivot\n",
        "for metric, df in pivot_dfs.items():\n",
        "    print(f'{metric}'); print(round(df, 2 if metric != 'Sharpe' else 3))\n"
      ],
      "metadata": {
        "id": "pLQAVpo_UnlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the first 25 relevant columns\n",
        "df_filtered = df_updated.iloc[:, :25]\n",
        "\n",
        "# Defining metrics and their colours for heat maps\n",
        "metrics = [\n",
        "    'AbnRet mean annual', 'Volatility annual', 'Sharpe', 'Intercept',\n",
        "    'Market', 'Size', 'Value', 'Quality', 'Investment'\n",
        "]\n",
        "colors = [\"#ff0000\", \"#ff6600\", \"#ffff00\", \"#99ff99\", \"#00cc00\"]\n",
        "n_bins = 100  # Use 100 discrete colors\n",
        "cmap_name = 'custom_heatmap'\n",
        "custom_cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
        "\n",
        "# Process each metric and generate heat maps\n",
        "for metric in metrics:\n",
        "    # Filtering the data relevant to the metric\n",
        "    temp_df = df_filtered.loc[metric].reset_index()\n",
        "\n",
        "    # Separating variables to obtain row and column categories\n",
        "    temp_df[['ME', 'BM']] = temp_df['index'].str.split(' ', expand=True)\n",
        "\n",
        "    # Delete the original index column\n",
        "    temp_df.drop(columns=['index'], inplace=True)\n",
        "\n",
        "    # Create a pivot matrix with ME as columns and BM as rows\n",
        "    pivot_df = temp_df.pivot(index='BM', columns='ME', values=metric)\n",
        "\n",
        "    # Create the heat matrix\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(pivot_df, annot=True, cmap=custom_cmap, cbar_kws={'label': f'{metric} Range'})\n",
        "    plt.title(f'{metric} Heatmap')\n",
        "    plt.show()\n",
        "# Dictionary to store the pivot DataFrames\n",
        "pivot_dfs = {}\n",
        "\n",
        "# Process each metric, extract data, create pivot and\n",
        "# store in dictionary\n",
        "for metric in metrics:\n",
        "    temp_df = df_filtered.loc[metric].reset_index()\n",
        "    temp_df[['ME', 'BM']] = temp_df['index'].str.split(' ', expand=True)\n",
        "    temp_df.drop(columns=['index'], inplace=True)\n",
        "    pivot_dfs[metric] = temp_df.pivot(index='BM', columns='ME', values=metric)\n",
        "\n",
        "# Print each rounded DataFrame pivot\n",
        "for metric, df in pivot_dfs.items():\n",
        "    print(f'{metric}'); print(round(df, 2 if metric != 'Sharpe' else 3))\n"
      ],
      "metadata": {
        "id": "sfnzmMMlUree"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}